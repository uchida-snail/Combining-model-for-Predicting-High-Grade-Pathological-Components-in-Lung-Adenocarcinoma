{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563864c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dba202d",
   "metadata": {},
   "source": [
    "# 1 feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8b4ec",
   "metadata": {},
   "source": [
    "# 1.1 Habitat Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb305163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from radiomics import featureextractor\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# 从YAML文件读取配置参数\n",
    "params_path = 'C:/Radiomics analysis/live.yaml'\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(params_path)\n",
    "\n",
    "# 文件夹路径\n",
    "roi_folder_path = 'C:/Users/21581/Desktop/AllRadiomics_feature/ICCfiles/ICC20live'  # ROI文件夹路径\n",
    "image_folder_path = 'C:/Users/21581/Desktop/AllRadiomics_feature/ICCfiles/ICC20'  # 原始图像文件夹路径\n",
    "output_folder = 'C:/Users/21581/Desktop/AllRadiomics_feature/ICCfiles/ICC20livefeature'  # 输出文件夹路径\n",
    "error_log_path = os.path.join(output_folder, 'error_log.csv')  # 错误记录文件路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 初始化错误记录列表\n",
    "error_log = []\n",
    "\n",
    "# 为每个label初始化一个空的DataFrame\n",
    "feature_dfs = {label_num: pd.DataFrame() for label_num in range(1, 10)}\n",
    "\n",
    "# 遍历原始图像文件夹的子文件夹，每个子文件夹以患者姓名命名\n",
    "for patient_name in os.listdir(image_folder_path):\n",
    "    patient_folder = os.path.join(image_folder_path, patient_name)\n",
    "    \n",
    "    # 检查路径是否为目录\n",
    "    if not os.path.isdir(patient_folder):\n",
    "        continue\n",
    "    \n",
    "    # 构建图像文件路径\n",
    "    image_file_path = os.path.join(patient_folder, 'image_path.nii')\n",
    "    \n",
    "    if not os.path.isfile(image_file_path):\n",
    "        print(f'Image file not found for patient: {patient_name}')\n",
    "        continue\n",
    "    \n",
    "    # 加载原始图像\n",
    "    image = sitk.ReadImage(image_file_path)\n",
    "    \n",
    "    for label_num in range(1, 10):\n",
    "        # 构建ROI文件的完整路径\n",
    "        roi_file_path = os.path.join(roi_folder_path, f'{patient_name}_habitat.nii.gz')\n",
    "        \n",
    "        if not os.path.isfile(roi_file_path):\n",
    "            print(f'ROI file not found for patient: {patient_name}, label: {label_num}')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 加载ROI标签图像\n",
    "            label_image = sitk.ReadImage(roi_file_path)\n",
    "            binary_label = sitk.BinaryThreshold(label_image, lowerThreshold=label_num, upperThreshold=label_num, insideValue=1, outsideValue=0)\n",
    "            \n",
    "            # 提取特征\n",
    "            features = extractor.execute(image, binary_label)\n",
    "            features_df = pd.Series(features)\n",
    "            \n",
    "            # 将患者姓名作为索引添加到DataFrame\n",
    "            features_df.name = patient_name\n",
    "            \n",
    "            # 将特征数据添加到相应label的DataFrame\n",
    "            feature_dfs[label_num] = feature_dfs[label_num].append(features_df)\n",
    "            \n",
    "            print(f'Extracted features for patient: {patient_name}, label: {label_num}')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to extract features for patient: {patient_name}, label: {label_num}. Error: {e}')\n",
    "            # 记录发生错误的文件\n",
    "            error_log.append({'Patient': patient_name, 'Label': label_num, 'Error': str(e)})\n",
    "\n",
    "# 保存每个label的特征到CSV文件\n",
    "for label_num, df in feature_dfs.items():\n",
    "    output_file_path = os.path.join(output_folder, f'features_label_{label_num}.csv')\n",
    "    df.to_csv(output_file_path)\n",
    "    print(f'Saved features for label {label_num} to {output_file_path}')\n",
    "\n",
    "# 保存错误日志到CSV文件\n",
    "error_log_df = pd.DataFrame(error_log)\n",
    "error_log_df.to_csv(error_log_path, index=False)\n",
    "print(f'Saved error log to {error_log_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1bccf",
   "metadata": {},
   "source": [
    "# 1.2 Peritumor&Primary Tumor Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11759c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#240316瘤周特征提取代码\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from radiomics import featureextractor\n",
    "\n",
    "# 瘤周特征提取\n",
    "image_dir = 'C:/Users/21581/Desktop/AllRadiomics_feature/ICCperitumor/1mm/image'\n",
    "roi_dir = 'C:/Users/21581/Desktop/AllRadiomics_feature/ICCperitumor/1mm/ROI'\n",
    "params_path = 'C:/Radiomics analysis/Peritumor.yaml'\n",
    "\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(params_path)\n",
    "\n",
    "patient_features = {}\n",
    "patients_with_no_labels = []  # 用于存储没有标签的病人ID\n",
    "\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.endswith('.nii'):\n",
    "        patient_id = image_file.rsplit('_', 1)[0]\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        roi_file = f'{patient_id}_roi.nii'\n",
    "        roi_path = os.path.join(roi_dir, roi_file)\n",
    "        if not os.path.exists(roi_path):\n",
    "            print(f'警告：未找到 {patient_id} 的ROI文件。')\n",
    "            patients_with_no_labels.append(patient_id)\n",
    "            continue  # 如果没有找到ROI文件，则跳过这个病人\n",
    "\n",
    "        try:\n",
    "            print(f'正在处理 {patient_id} 的特征提取...')\n",
    "            features = extractor.execute(image_path, roi_path)\n",
    "            patient_features[patient_id] = features\n",
    "        except Exception as e:\n",
    "            if 'No labels found' in str(e):\n",
    "                print(f'警告：{patient_id} 的掩膜中没有找到标签。')\n",
    "                patients_with_no_labels.append(patient_id)\n",
    "            else:\n",
    "                print(f'警告：处理 {patient_id} 时出现其他错误。错误信息：{e}')\n",
    "\n",
    "# 导出没有标签的病人列表到CSV文件\n",
    "if patients_with_no_labels:\n",
    "    df_no_labels = pd.DataFrame(patients_with_no_labels, columns=['PatientID'])\n",
    "    df_no_labels.to_csv('C:/Users/21581/Desktop/patients_with_no_labels.csv', index=False)\n",
    "    print('导出没有掩膜标签的患者列表到CSV文件。')\n",
    "\n",
    "# 导出特征到CSV文件\n",
    "if patient_features:\n",
    "    output_path = 'C:/Users/21581/Desktop/output_features.csv'\n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['PatientID'] + list(next(iter(patient_features.values())).keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for patient_id, features in patient_features.items():\n",
    "            row = {'PatientID': patient_id}\n",
    "            row.update(features)\n",
    "            writer.writerow(row)\n",
    "    print(f\"特征提取完毕，并已导出到CSV文件：{output_path}\")\n",
    "else:\n",
    "    print(\"没有特征被提取，请检查文件路径和格式。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea3bb3",
   "metadata": {},
   "source": [
    "# 2 feature preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c44cb",
   "metadata": {},
   "source": [
    "# BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf434c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BorderlineSMOTE 重采样拆分\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import BorderlineSMOTE  # 修改这里导入BorderlineSMOTE\n",
    "import os\n",
    "\n",
    "# 设定CSV文件所在的目录路径\n",
    "directory_path = 'C:/Users/21581/Desktop/analysis0430'\n",
    "output_path = 'C:/Users/21581/Desktop/analysis0430/trainandtest'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)  # 如果输出目录不存在，创建它\n",
    "\n",
    "# CSV文件列表\n",
    "csv_files = [\n",
    "    'filtered0mm.csv',\n",
    "    'filtered1mm.csv',\n",
    "    'filtered2mm.csv',\n",
    "    'filtered3mm.csv',\n",
    "    'filtered4mm.csv',\n",
    "    'filtered5mm.csv',\n",
    "    'CTCclinical.csv',\n",
    "    'live.csv',\n",
    "]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(directory_path, csv_file), encoding='ISO-8859-1')\n",
    "\n",
    "    # 分离特征和标签\n",
    "    X = df.iloc[:, 2:]  # 特征列\n",
    "    y = df.iloc[:, 1]   # 标签列\n",
    "    patient_info = df.iloc[:, :1]  # 患者信息列\n",
    "    \n",
    "    # 拆分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=420)\n",
    "    \n",
    "    # 应用BorderlineSMOTE进行过采样\n",
    "    smote = BorderlineSMOTE(random_state=420)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # 创建与重采样后数据相对应的患者信息，新样本填充默认值 'aaaa'\n",
    "    patient_info_resampled = pd.DataFrame('aaaa', index=X_train_resampled.index, columns=['info'])\n",
    "    # 更新原始样本的患者信息\n",
    "    patient_info_resampled.loc[X_train.index, 'info'] = patient_info.loc[X_train.index].squeeze()\n",
    "\n",
    "    # 初始化标准化器\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 标准化训练数据和测试数据\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 将标准化后的数据转换回DataFrame\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_resampled.columns, index=X_train_resampled.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # 合并患者信息、标准化后的特征和标签\n",
    "    train_data = pd.concat([patient_info_resampled, X_train_scaled, y_train_resampled], axis=1)\n",
    "    test_data = pd.concat([patient_info.loc[X_test.index], X_test_scaled, y_test], axis=1)\n",
    "    \n",
    "    # 保存到CSV文件\n",
    "    train_output_file = os.path.join(output_path, f'train_{csv_file}')\n",
    "    test_output_file = os.path.join(output_path, f'test_{csv_file}')\n",
    "    train_data.to_csv(train_output_file, index=False)\n",
    "    test_data.to_csv(test_output_file, index=False)\n",
    "    print(f'Processed {csv_file}: Train and test CSV files saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1ecc4",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu, ttest_ind, shapiro\n",
    "\n",
    "# 输入文件夹和输出文件夹的路径\n",
    "input_folder = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data'\n",
    "output_folder = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/p_value_filtered'\n",
    "\n",
    "# 如果输出文件夹不存在，创建输出文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 准备记录删除特征数量的列表\n",
    "features_removed = []\n",
    "\n",
    "def process_files(train_file, test_file):\n",
    "    try:\n",
    "        # 读取训练集和测试集文件\n",
    "        train_df = pd.read_csv(os.path.join(input_folder, train_file))\n",
    "        test_df = pd.read_csv(os.path.join(input_folder, test_file))\n",
    "        \n",
    "        # 假定第一列是标签\n",
    "        y = train_df.iloc[:, 0]\n",
    "        p_values = []\n",
    "        \n",
    "        # 对每个特征进行正态性检验和适当的统计测试\n",
    "        for col in train_df.columns[1:]:  # 排除第一列标签\n",
    "            # 正态性检验\n",
    "            p_normal_train = shapiro(train_df[train_df.iloc[:, 0] == 1][col])[1]\n",
    "            p_normal_test = shapiro(train_df[train_df.iloc[:, 0] == 0][col])[1]\n",
    "            \n",
    "            # 检验是否满足正态分布\n",
    "            if p_normal_train > 0.05 and p_normal_test > 0.05:\n",
    "                # 使用t检验\n",
    "                stat, p = ttest_ind(train_df[train_df.iloc[:, 0] == 1][col], \n",
    "                                    train_df[train_df.iloc[:, 0] == 0][col])\n",
    "            else:\n",
    "                # 使用Mann-Whitney U检验\n",
    "                stat, p = mannwhitneyu(train_df[train_df.iloc[:, 0] == 1][col], \n",
    "                                       train_df[train_df.iloc[:, 0] == 0][col])\n",
    "            p_values.append(p)\n",
    "        \n",
    "        # 将p值转换为Series对象，并筛选p值小于0.05的特征\n",
    "        p_values_series = pd.Series(p_values, index=train_df.columns[1:])\n",
    "        to_keep = p_values_series[p_values_series < 0.05].index.tolist()\n",
    "        \n",
    "        # 记录删除的特征数量\n",
    "        removed_features_count = len(train_df.columns[1:]) - len(to_keep)\n",
    "        features_removed.append({'file': train_file, 'removed_features_count': removed_features_count})\n",
    "        \n",
    "        # 保留p值小于0.05的特征和标签列\n",
    "        train_df_reduced = train_df[['Pathology'] + to_keep]\n",
    "        test_df_reduced = test_df[['Pathology'] + to_keep]\n",
    "        \n",
    "        # 保存筛选后的数据集\n",
    "        train_df_reduced.to_csv(os.path.join(output_folder, f'reduced_{train_file}'), index=False)\n",
    "        test_df_reduced.to_csv(os.path.join(output_folder, f'reduced_{test_file}'), index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理{train_file}时出错：{e}\")\n",
    "\n",
    "# 训练集和测试集文件名列表\n",
    "# 训练集和测试集文件名列表\n",
    "\n",
    "train_files = ['train_processed_features_label_1.csv', 'train_processed_features_label_2.csv', 'train_processed_features_label_3.csv',\n",
    "               'train_processed_features_label_4.csv', 'train_processed_features_label_5.csv', 'train_processed_features_label_6.csv',\n",
    "               'train_processed_features_label_7.csv', 'train_processed_features_label_8.csv', 'train_processed_features_label_9.csv']\n",
    "test_files = ['test_processed_features_label_1.csv', 'test_processed_features_label_2.csv', 'test_processed_features_label_3.csv',\n",
    "              'test_processed_features_label_4.csv', 'test_processed_features_label_5.csv', 'test_processed_features_label_6.csv',\n",
    "              'test_processed_features_label_7.csv', 'test_processed_features_label_8.csv', 'test_processed_features_label_9.csv']\n",
    "\n",
    "\n",
    "# 处理每个文件对\n",
    "for train_file, test_file in zip(train_files, test_files):\n",
    "    process_files(train_file, test_file)\n",
    "\n",
    "# 将删除的特征数量的数据保存为CSV文件\n",
    "features_removed_df = pd.DataFrame(features_removed)\n",
    "features_removed_df.to_csv(os.path.join(output_folder, 'removed_features_summary.csv'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f63780",
   "metadata": {},
   "source": [
    "# Spearman Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 输入文件夹和输出文件夹的路径\n",
    "input_folder = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/p_value_filtered'\n",
    "output_folder = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/spearman'\n",
    "\n",
    "# 如果输出文件夹不存在，创建输出文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 用于记录每个文件删除特征数量的字典\n",
    "features_removed = {}\n",
    "\n",
    "def process_files(train_file, test_file):\n",
    "    try:\n",
    "        # 读取训练集和测试集文件\n",
    "        train_df = pd.read_csv(os.path.join(input_folder, train_file))\n",
    "        test_df = pd.read_csv(os.path.join(input_folder, test_file))\n",
    "        \n",
    "        # 计算Spearman秩相关性矩阵\n",
    "        corr_matrix = train_df.corr(method='spearman').abs()\n",
    "        \n",
    "        # 选择相关性矩阵的上三角部分，避免重复特征对\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # 找到相关系数大于0.75的特征列\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "        \n",
    "        # 记录删除特征的数量\n",
    "        features_removed[train_file] = len(to_drop)\n",
    "        \n",
    "        # 删除这些列\n",
    "        train_df_reduced = train_df.drop(columns=to_drop)\n",
    "        test_df_reduced = test_df.drop(columns=to_drop)\n",
    "        \n",
    "        # 保存筛选后的数据集\n",
    "        train_df_reduced.to_csv(os.path.join(output_folder, f'reduced_{train_file}'), index=False)\n",
    "        test_df_reduced.to_csv(os.path.join(output_folder, f'reduced_{test_file}'), index=False)\n",
    "        \n",
    "        # 如果需要，生成并保存相关性热图\n",
    "        if os.environ.get('GENERATE_HEATMAPS', '0') == '1':\n",
    "            plt.figure(figsize=(20, 16))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "            plt.title(f'{train_file}的相关性矩阵')\n",
    "            plt.savefig(os.path.join(output_folder, f'heatmap_{train_file}.png'))\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"处理{train_file}时出错：{e}\")\n",
    "    \n",
    "# 训练集和测试集文件名列表\n",
    "    \n",
    "train_files = ['reduced_train_processed_features_label_1.csv', 'reduced_train_processed_features_label_2.csv', 'reduced_train_processed_features_label_3.csv',\n",
    "               'reduced_train_processed_features_label_4.csv', 'reduced_train_processed_features_label_5.csv', 'reduced_train_processed_features_label_6.csv',\n",
    "               'reduced_train_processed_features_label_7.csv', 'reduced_train_processed_features_label_8.csv', 'reduced_train_processed_features_label_9.csv']\n",
    "test_files = ['reduced_test_processed_features_label_1.csv', 'reduced_test_processed_features_label_2.csv', 'reduced_test_processed_features_label_3.csv',\n",
    "              'reduced_test_processed_features_label_4.csv', 'reduced_test_processed_features_label_5.csv', 'reduced_test_processed_features_label_6.csv',\n",
    "              'reduced_test_processed_features_label_7.csv', 'reduced_test_processed_features_label_8.csv', 'reduced_test_processed_features_label_9.csv']\n",
    "    \n",
    "    \n",
    "# 处理每个文件对\n",
    "for train_file, test_file in zip(train_files, test_files):\n",
    "    process_files(train_file, test_file)\n",
    "    \n",
    "# 将删除特征的数量的数据保存为CSV文件\n",
    "features_removed_df = pd.DataFrame(list(features_removed.items()), columns=['File', 'Removed Features Count'])\n",
    "features_removed_df.to_csv(os.path.join(output_folder, 'removed_features_summary_spearman.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2cecd",
   "metadata": {},
   "source": [
    "# 3 Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9baa5",
   "metadata": {},
   "source": [
    "# Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0606ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_SEED = 5\n",
    "\n",
    "# 数据集路径\n",
    "input_dir = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/combine/1'\n",
    "output_dir = 'C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/combine/1/11'\n",
    "\n",
    "# 确保输出目录存在\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 获取训练和测试文件路径\n",
    "train_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if 'train' in f and f.endswith('.csv')]\n",
    "test_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if 'test' in f and f.endswith('.csv')]\n",
    "\n",
    "X_trains, y_trains, X_tests, y_tests = [], [], [], []\n",
    "\n",
    "# 加载数据集\n",
    "for train_path, test_path in zip(train_files, test_files):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    X_trains.append(train_df.drop('Pathology', axis=1).values)\n",
    "    y_trains.append(train_df['Pathology'].values)\n",
    "    X_tests.append(test_df.drop('Pathology', axis=1).values)\n",
    "    y_tests.append(test_df['Pathology'].values)\n",
    "\n",
    "# 确保训练和测试数据集长度一致\n",
    "def ensure_same_length(X_list, y_list):\n",
    "    min_length = min(len(X) for X in X_list)\n",
    "    X_list = [X[:min_length] for X in X_list]\n",
    "    y_list = [y[:min_length] for y in y_list]\n",
    "    return X_list, y_list\n",
    "\n",
    "X_trains, y_trains = ensure_same_length(X_trains, y_trains)\n",
    "X_tests, y_tests = ensure_same_length(X_tests, y_tests)\n",
    "\n",
    "# 基础模型列表，增加正则化强度，并添加更多基础模型\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=100, penalty='l2', C=0.001, random_state=RANDOM_SEED)),  # 增加正则化\n",
    "    ('rf', RandomForestClassifier(n_estimators=30, max_depth=2, random_state=RANDOM_SEED)),     # 减少模型复杂度\n",
    "    ('svc', SVC(probability=True, C=0.001, kernel='linear', random_state=RANDOM_SEED)),         # 增加正则化\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=20)),                                             # 增加K值，减少过拟合\n",
    "    ('gbc', GradientBoostingClassifier(n_estimators=30, learning_rate=0.05, random_state=RANDOM_SEED))  # 减少复杂度\n",
    "]\n",
    "\n",
    "# 使用交叉验证生成元特征\n",
    "def get_meta_features(model, X_trains, y_trains, X_tests, n_splits=5, n_repeats=10):\n",
    "    n_samples = X_trains[0].shape[0]\n",
    "    meta_train = np.zeros((n_samples,))\n",
    "    meta_test_list = []\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, val_index in rskf.split(X_trains[0], y_trains[0]):\n",
    "        fold_meta_train = np.zeros((len(val_index),))\n",
    "        fold_meta_test = np.zeros((len(X_tests[0]), len(X_tests)))\n",
    "        for i, (X_train, y_train, X_test) in enumerate(zip(X_trains, y_trains, X_tests)):\n",
    "            X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "            y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "            model.fit(X_tr, y_tr)\n",
    "            fold_meta_train += model.predict_proba(X_val)[:, 1]\n",
    "            fold_meta_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        meta_train[val_index] = fold_meta_train / len(X_trains)\n",
    "        meta_test_list.append(np.mean(fold_meta_test, axis=1))\n",
    "    \n",
    "    meta_test = np.mean(meta_test_list, axis=0)\n",
    "    \n",
    "    # 检查模型是否选择了特征\n",
    "    selected_features = False\n",
    "    if hasattr(model, 'coef_'):\n",
    "        selected_features = np.any(model.coef_ != 0)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        selected_features = np.any(model.feature_importances_ != 0)\n",
    "    \n",
    "    return meta_train, meta_test, selected_features\n",
    "\n",
    "# 初始化元特征数组\n",
    "meta_train_list = []\n",
    "meta_test_list = []\n",
    "selected_models = []\n",
    "\n",
    "for name, model in base_models:\n",
    "    meta_train, meta_test, selected_features = get_meta_features(model, X_trains, y_trains, X_tests)\n",
    "    meta_train_list.append(meta_train)\n",
    "    meta_test_list.append(meta_test)\n",
    "    if selected_features:\n",
    "        selected_models.append(name)\n",
    "\n",
    "# 汇总元特征\n",
    "meta_train = np.column_stack(meta_train_list)\n",
    "meta_test = np.column_stack(meta_test_list)\n",
    "\n",
    "# 保存元特征到CSV文件\n",
    "pd.DataFrame(meta_train).to_csv(os.path.join(output_dir, 'meta_features_train.csv'), index=False)\n",
    "pd.DataFrame(meta_test).to_csv(os.path.join(output_dir, 'meta_features_test.csv'), index=False)\n",
    "\n",
    "# 打印哪些模型选择了特征\n",
    "print(\"Selected models with features:\", selected_models)\n",
    "\n",
    "# 构建StackingClassifier，使用LogisticRegression作为元模型并进行贝叶斯优化\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(random_state=RANDOM_SEED, penalty='l2', C=0.01),\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# 定义贝叶斯优化的参数空间\n",
    "param_space = {\n",
    "    'final_estimator__C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'final_estimator__penalty': ['l1', 'l2'],\n",
    "    'final_estimator__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# 使用BayesSearchCV进行贝叶斯优化\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=stacking_model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,\n",
    "    cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=RANDOM_SEED),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 训练StackingClassifier\n",
    "bayes_search.fit(meta_train, y_trains[0])\n",
    "\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# 预测\n",
    "train_probabilities = best_model.predict_proba(meta_train)[:, 1]\n",
    "test_probabilities = best_model.predict_proba(meta_test)[:, 1]\n",
    "\n",
    "# 计算ROC曲线数据\n",
    "fpr_train, tpr_train, _ = roc_curve(y_trains[0], train_probabilities)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_tests[0], test_probabilities)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, label='Train ROC curve (area = %0.2f)' % roc_auc_score(y_trains[0], train_probabilities))\n",
    "plt.plot(fpr_test, tpr_test, label='Test ROC curve (area = %0.2f)' % roc_auc_score(y_tests[0], test_probabilities))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 打印评估指标\n",
    "y_train_pred = (train_probabilities > 0.5).astype(int)\n",
    "y_test_pred = (test_probabilities > 0.5).astype(int)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_trains[0], y_train_pred))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_tests[0], y_test_pred))\n",
    "print(\"Train F1 Score: \", f1_score(y_trains[0], y_train_pred))\n",
    "print(\"Test F1 Score: \", f1_score(y_tests[0], y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57efa3",
   "metadata": {},
   "source": [
    "# Stacking Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载第一层的元特征矩阵\n",
    "meta_train = pd.read_csv('C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/results/predicted_results/meta_featuressss/meta_features_train.csv')\n",
    "meta_test = pd.read_csv('C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/results/predicted_results/meta_featuressss/meta_features_test.csv')\n",
    "\n",
    "# 加载临床特征数据\n",
    "train_clinical_df = pd.read_csv('C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/results/predicted_results/meta_featuressss/trainCTCCTC.csv')  # 示例路径\n",
    "test_clinical_df = pd.read_csv('C:/Users/21581/Desktop/240517live_selection/bspline88/subtumor_data/results/predicted_results/meta_featuressss/testCTCCTC.csv')  # 示例路径\n",
    "\n",
    "# 获取标签\n",
    "y_train = train_clinical_df['Pathology'].values\n",
    "y_test = test_clinical_df['Pathology'].values\n",
    "\n",
    "# 融合元特征和临床特征\n",
    "X_train_combined = np.column_stack((meta_train.values, train_clinical_df[['Tumor_diameter', 'CTC']].values))\n",
    "X_test_combined = np.column_stack((meta_test.values, test_clinical_df[['Tumor_diameter', 'CTC']].values))\n",
    "\n",
    "# 使用 StackingClassifier 作为第一层模型\n",
    "second_layer_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('svc', SVC(probability=True)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('gbc', GradientBoostingClassifier())\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(penalty='l1', solver='liblinear', C=1.0),  # 使用L1正则化的Logistic回归\n",
    "    passthrough=True,\n",
    "    cv=5\n",
    ")\n",
    "second_layer_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# 预测并评估训练集\n",
    "train_predictions = second_layer_model.predict_proba(X_train_combined)[:, 1]\n",
    "train_auc = roc_auc_score(y_train, train_predictions)\n",
    "train_fpr, train_tpr, _ = roc_curve(y_train, train_predictions)\n",
    "train_accuracy = accuracy_score(y_train, (train_predictions > 0.5).astype(int))\n",
    "\n",
    "# 预测并评估测试集\n",
    "test_predictions = second_layer_model.predict_proba(X_test_combined)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_predictions)\n",
    "test_fpr, test_tpr, _ = roc_curve(y_test, test_predictions)\n",
    "test_accuracy = accuracy_score(y_test, (test_predictions > 0.5).astype(int))\n",
    "\n",
    "# 寻找最佳阈值以优化F1 Score\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_predictions)\n",
    "f1_scores = 2 * precision * recall / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "test_f1 = np.max(f1_scores)\n",
    "\n",
    "# 使用最佳阈值重新计算F1 Score\n",
    "train_f1 = f1_score(y_train, (train_predictions > best_threshold).astype(int))\n",
    "test_f1 = f1_score(y_test, (test_predictions > best_threshold).astype(int))\n",
    "\n",
    "# 打印评估指标\n",
    "print(f\"Train ROC AUC: {train_auc:.2f}\")\n",
    "print(f\"Test ROC AUC: {test_auc:.2f}\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Train F1 Score: {train_f1:.2f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.2f}\")\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "\n",
    "# 绘制每条曲线并添加数据点\n",
    "plt.plot(train_fpr, train_tpr, color='red', label=f'Training Dataset AUC={train_auc:.2f}')\n",
    "plt.scatter(train_fpr, train_tpr, color='red', s=10)  # s参数控制点的大小\n",
    "plt.plot(test_fpr, test_tpr, color='orange', label=f'Test Dataset AUC={test_auc:.2f}')\n",
    "plt.scatter(test_fpr, test_tpr, color='orange', s=10)  # s参数控制点的大小\n",
    "\n",
    "# 绘制对角线\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# 设置图形标签\n",
    "plt.xlabel('False positive rate (1-specificity)')\n",
    "plt.ylabel('True positive rate (sensitivity)')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "# 设置图例位置\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806d387",
   "metadata": {},
   "source": [
    "# ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 定义文件路径和自定义标签\n",
    "file_paths = [\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combinelivetest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combine0mmtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combine1mmtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combine3mmtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combine5mmtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/onlyCTCtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/onlylivetest.pkl',\n",
    "]\n",
    "labels = [\n",
    "    'Habitat_CTC',\n",
    "    'Primary Lesion',\n",
    "    'Prei_1mm',\n",
    "    'Prei_3mm',\n",
    "    'Prei_5mm',\n",
    "    'CTC_Clinical',\n",
    "    'Habitat',\n",
    "]\n",
    "\n",
    "# 初始化列表存储每个pkl文件的内容\n",
    "roc_data = []\n",
    "\n",
    "# 加载每个pkl文件\n",
    "for path in file_paths:\n",
    "    roc_data.append(joblib.load(path))\n",
    "\n",
    "# 计算AUC的95%置信区间\n",
    "def auc_confidence_interval(y_true, y_scores, confidence_level=0.95):\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    n1 = sum(y_true == 1)\n",
    "    n2 = sum(y_true == 0)\n",
    "    q1 = auc / (2 - auc)\n",
    "    q2 = 2 * auc ** 2 / (1 + auc)\n",
    "    auc_var = (auc * (1 - auc) +\n",
    "               (n1 - 1) * (q1 - auc ** 2) +\n",
    "               (n2 - 1) * (q2 - auc ** 2)) / (n1 * n2)\n",
    "    auc_std = np.sqrt(auc_var)\n",
    "    z = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "    lower_bound = auc - z * auc_std\n",
    "    upper_bound = auc + z * auc_std\n",
    "    return auc, lower_bound, upper_bound\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "\n",
    "# 遍历加载的ROC数据，并绘制每条曲线\n",
    "for idx, (fpr, tpr, auc, y_true, y_scores) in enumerate(roc_data):\n",
    "    # 计算AUC的95%置信区间\n",
    "    auc, lower_bound, upper_bound = auc_confidence_interval(y_true, y_scores)\n",
    "    label = f'{labels[idx]} AUC={auc:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})'\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "# 绘制对角线\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# 设置图形标签\n",
    "plt.xlabel('False positive rate (1-specificity)')\n",
    "plt.ylabel('True positive rate (sensitivity)')\n",
    "plt.title('Combined ROC Curve')\n",
    "\n",
    "# 设置图例位置\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# 保存图形到指定路径\n",
    "save_path = 'C:/Users/21581/Desktop/ROCPKL/combined_roc_curve241130.pdf'\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397e3c",
   "metadata": {},
   "source": [
    "# DCA Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 你提供的函数\n",
    "def calculate_net_benefit_model(thresh_group, y_pred_score, y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        if thresh == 0 or thresh == 1:\n",
    "            net_benefit_model = np.append(net_benefit_model, 0)\n",
    "            continue\n",
    "        y_pred_label = y_pred_score > thresh\n",
    "        tn, fp, fn, tp = confusion_matrix(y_label, y_pred_label).ravel()\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit_model = np.append(net_benefit_model, net_benefit)\n",
    "    return net_benefit_model\n",
    "\n",
    "def calculate_net_benefit_all(thresh_group, y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_label, y_label).ravel()\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        if thresh == 0 or thresh == 1:\n",
    "            net_benefit_all = np.append(net_benefit_all, 0)\n",
    "            continue\n",
    "        net_benefit = (tp / total) - (tn / total) * (thresh / (1 - thresh))\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)\n",
    "    return net_benefit_all\n",
    "\n",
    "def plot_DCA(ax, thresh_group, net_benefit_model, label, add_treat_labels=False):\n",
    "    # Plot\n",
    "    ax.plot(thresh_group, net_benefit_model, label=f'{label}')\n",
    "\n",
    "    # Fill，显示出模型较于treat all和treat none好的部分\n",
    "    y2 = np.maximum(calculate_net_benefit_all(thresh_group, y_true), 0)\n",
    "    y1 = np.maximum(net_benefit_model, y2)\n",
    "    ax.fill_between(thresh_group, y1, y2, alpha=0.2)\n",
    "\n",
    "    return ax\n",
    "\n",
    "# 加载测试集的pkl文件，假设每个模型的文件路径不同\n",
    "model_paths = [\n",
    "    'C:/Users/21581/Desktop/ROCPKL/combinelivetrain.pkl',\n",
    "    # 'C:/Users/21581/Desktop/ROCPKL/combine0mmtest.pkl',\n",
    "    # 'C:/Users/21581/Desktop/ROCPKL/combine1mmtest.pkl',\n",
    "    # 'C:/Users/21581/Desktop/ROCPKL/combine3mmtest.pkl',\n",
    "    # 'C:/Users/21581/Desktop/ROCPKL/combine5mmtest.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/onlyCTCtrain.pkl',\n",
    "    'C:/Users/21581/Desktop/ROCPKL/onlylivetrain.pkl',\n",
    "]\n",
    "\n",
    "# 自定义模型名称\n",
    "model_names = [\n",
    "    'Habitat_CTC',\n",
    "    'CTC',\n",
    "    'Habitat'\n",
    "]\n",
    "\n",
    "# 读取所有模型的数据\n",
    "models_data = [joblib.load(path) for path in model_paths]\n",
    "\n",
    "# 设置阈值范围\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# 初始化图形\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 计算并绘制每个模型的DCA曲线\n",
    "for i, (y_true, predictions) in enumerate([(data[3], data[4]) for data in models_data]):\n",
    "    net_benefit_model = calculate_net_benefit_model(thresholds, predictions, y_true)\n",
    "    add_treat_labels = (i == 0)\n",
    "    ax = plot_DCA(ax, thresholds, net_benefit_model, label=model_names[i], add_treat_labels=add_treat_labels)\n",
    "\n",
    "# 绘制 Treat all 和 Treat none 的基准线\n",
    "net_benefit_all = calculate_net_benefit_all(thresholds, y_true)\n",
    "ax.plot(thresholds, net_benefit_all, color='black', linestyle='--', label='Treat all')\n",
    "ax.plot((0, 1), (0, 0), color='black', linestyle=':', label='Treat none')\n",
    "\n",
    "# 图形配置\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.15, net_benefit_model.max() + 0.15)\n",
    "ax.set_xlabel('Threshold Probability', fontdict={'family': 'Arial', 'fontsize': 16})\n",
    "ax.set_ylabel('Net Benefit', fontdict={'family': 'Arial', 'fontsize': 16})\n",
    "ax.grid('major')\n",
    "ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = handles[:-2] + handles[-2:][::-1]\n",
    "labels = labels[:-2] + labels[-2:][::-1]\n",
    "ax.legend(handles, labels, loc='upper right')\n",
    "\n",
    "plt.title('Decision Curve Analysis')\n",
    "save_path = 'C:/Users/21581/Desktop/ROCPKL/trainDCA.pdf'\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
